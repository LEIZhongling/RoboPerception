# JetBot AI 关于小车实验的心得体会

## 实验一：组装小车

在组装小车之前首先要明确各零部件是什么，如电机，金属盒，天线，摄像头，扩展板，Jetson nano核心板和无线网卡等重要的零部件。在安装的过程中，虽然是按照过程一步步来，但是其中最令人困扰的就是螺旋丝的拧的过程，因为螺丝很小，加上我手很笨拙，安装时属实费力。同时根据设计图组装小车的过程中叶遇到了一些问题。镜头插槽安装方向反了，导致无法和支架安装；在插入处理芯片的时候没有合适的工具，导致芯片插入不牢固；镜头支架固定不牢固，导致小车运动时镜头不稳。至于最不应该犯的错误是风扇方向装反了，导致无法固定。不过问题都在同学和小组成员的帮助下解决了。但是按照过程来问题也不是很大，还是轻松完成。

## 实验二：安装镜像

在老师的帮助下，在教室的主机上烧写一份镜像在sd卡上，然后将其插入到核心板上，之后确保Jetson Nano没有问题，组装后让小车连接WiFi，在浏览器上连接小车更新软件。做完实验二的步骤后，感觉实验才刚刚开始，能够成功进入JupyterLab网站，缺点是教室网很慢，很耽误时间。还有就是写入的速度很令人着急，所以找了写入与拷贝的区别——

拷贝：拷贝就相当于复制，区别不大。

写入：写入不仅仅复制了文件，同样保留了文件之间的组成形式，保留了功能。我们知道，以iso系统安装盘为例，电脑可以通过系统盘启动安装，但是你复制光盘的东西到U盘就不能启动安装，你需要通过软件写入，然后U盘里面就既包含文件又包含引导系统，可以当做启动光盘来装系统。

## 实验三：电机驱动

这个部分的实验主要就是跑代码，不是在运行就是在运行的路上。我们是在别人已经写好的代码上运行，没有任何难度，缺点是需要慢慢的运行，运行一段需要等几秒才能运行下一段，不然这个小车反应不过来就会动不了。这个实验主要就是小车的移动和转向。驱动电机是最基础的指令，可以通过自调speed来实现控速，经过测试速度区间在0.0-1.0，速度分级并不明显可见硬件指令的设计并没有太细，如上描述最小识别单位级是10^(-1)。顺序跑完代码感觉做基本的调试还是可能的，最后stop()函数——小车停转。

## 实验四：远程遥控

在做这个实验的时候，手柄出了问题。最开始是检查不到遥控手柄，然后检查了电池，发现电池其实是有点松了，解决了这个问题后就成功的检查到了手柄，还有就是手柄按键的index说实话现在还是没有记清楚。运行代码的时候要注意代码里的index值和你的按键的index值是否对应。不然可能按了按键没有反应。在实际连接操作的过程中发现十字键控制是左键为正方向，即左向前，右向后，下向左，上向右，当然这是代码中按键映射的问题，是可以调整的。

## 实验五：自主避障

这是实验过程中第一次明显感受到的阻碍，也是重新设置网络的开始。网络真的对于图像识别有很大的作用，小车作为一个独立的单位并且需要移动，自然不能用串口对其作指令传输和信息采集，所以局域网的速度是决定小车识别速度的原因之一，这一点我深有体会。糟糕的网络问题会让图像显示有几分钟的延迟，这注定我们第一次的实验不可能有结果。在调整好网络之后，模型下载，素材采集和学习过程都能很快的完成。由此发现，自动驾驶确实需要网络和其他硬件的发展才可能实现。

## 实验六：目标跟踪

代码流程大同小异，先下载学习模型，这一步还省了一步素材采集。实际运行效果看起来还不错，可以识别桌子腿，鞋子和其他障碍物，但遗憾的是唯一不能识别的是我们用来测试的白色乒乓球，可能颜色和大小的问题导致识别失败，具体原因不明，block识别条也没有反应，只能把问题归结于学习程序。


## 实验七：目标巡线

这步我们用的黑色鼠标线作为寻线标准，通过素材识别获得了100张采集图，和上面的实验类似，通过学习程序学习之后。当我们实际运行时发现了问题。我们按照惯性思维为他设计了寻线路径，他是条直线。所以当线变得曲折时他停了下来，这是在是我们的失败，采集了100张直线实际上只采集了一张图片，我们不得不再一次进行图片采集。这是个很憨批的错误，然而当时做的时候忘记提醒了。因此我觉得训练模型是真的复杂，情况不能过于单一，最好是有各种情况，但是条件也不是很允许。

## 实验八：ROS
新认识的系统必然导致没见过的报错，ros的摸索和初始centos和liunx一样遇到各种这样的指令问题，提供的代码直接运行出现各种各样的错误已经见怪不怪了，这部分的实验进度缓慢我想也是可以理解的。没见过，多试试，总没错误的。