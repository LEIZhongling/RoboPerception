智能182 王世松 201809057 课设报告

![](/images/1.png)

 # <center>车载信息处理课程报告</center>
 &nbsp;
  &nbsp;
   &nbsp;
    &nbsp;
     &nbsp;
      &nbsp;

<center>课程：<u>_____________车载信息处理_____________</u></center>
<center>授课教师：<u>____________龚家元________________</u></center>
<center>报告名称：<u>基于OpenVINO的图像处理、检测与识别_</u></center>
<center>学号：<u>_____________201809057_______________</u></center>
<center>姓名：<u>______________王世松__________________</u></center>
<center>报告完成时间：<u>_______2020年12月29日__________</u></center>



 &nbsp;
  &nbsp;
   &nbsp;
    &nbsp;
    &nbsp;
        &nbsp;
            &nbsp;



 ## 1 实验目的
 &nbsp; 本实验是基于OpenVINO的图像处理、检测与识别，实现对行人目标进行感知。在自动驾驶领域中，车载视觉摄像头对前方道路行人的动作侦测十分重要，帮助车辆进行行人动作的判断以控制实现车辆的制动与通过动作。当前，中国的道路条件错综复杂，从大型车辆中突然窜出的人或人驾驶的车辆造成的交通事故被称为“鬼探头”，是诱发交通事故的主要诱因之一。因此，为确保车内乘员与外部人员的共同安全，降低不必要发生的交通事故的发生概率，基于车载摄像头与相关算法对行人动作进行感知预判十分重要。
&nbsp;
 ![](images/intro.jpg)
<center>（图1.1 鬼探头示意图）</center>
&nbsp;

&nbsp;从2014年开始，以欧洲E-NCAP新车评估测试为首的非盈利组织开始倡导各国汽车安全组织将汽车主动安全方面进行评分化，纳入汽车安全性评价且占较大评分比例。由此部分未装配主动安全配置的车型在安全评定中将十分吃亏，此项新规足见欧洲汽车组织、欧洲汽车企业、欧洲汽车消费者对于汽车主动安全以及未来汽车发展趋势的重视，正因如此，装备毫米波雷达+摄像头初步实现L2级别自动驾驶的汽车越来越多，L2级别自动驾驶的运用越来越成熟可靠（法律法规的推陈出新也推动了自动驾驶汽车行业的发展）。
&nbsp;
![](/images/ENCAP.jpg)
<center>（图1.2 E-NCAP测试汽车针对行人感知的主动安全系统）</center>
            &nbsp;

## 2 实验过程

#### 1.编译 build_demos_msvc.bat 文件
```
cd C:\IntelTools\openvino\openvino_2021\deployment_tools\inference_engine\demos
```

```
输入 build_demos_msvc.bat 编译脚本文件
```
![](./Images/Visual_Studio.png)
<center>图1 编译脚本文件</center>

#### 2.执行完脚本之后，会在文档目录下生成一个 Intel 的文件夹，进入 Intel 文件夹下的 OpenVINO\omz_demos_build 文件夹，使用 VS2017 打开 Demos.sln 文件。

![](Images/Demo.png)
<center>图2 Demo.sln</center>

- 进入 Visual Studio 2019 后，右键点击 ALL_BUILD ，然后点击生成，开始进行编译

![](Images/complier.png)
<center>图3 complier</center>

- 在电脑的这个路径下：
```
C:\Users\NewAdmin\Documents\Intel\OpenVINO\omz_demos_build\intel64\Debug
```
可以看到有许多编译出来的文件，这里我们找到实验的human_pose_estimation_demo.exe程序

![](Images/complier_exe.png)
<center>图4 complier_exe</center>
#### 3.执行exe程序的准备工作
- 添加环境变量
```
C:\IntelTools\openvino\openvino_2021\opencv\bin
C:\IntelTools\openvino\openvino_2021\deployment_tools\inference_engine\bin\intel64\Debug
C:\IntelTools\openvino\openvino_2021\deployment_tools\inference_engine\bin\intel64\Release
C:\IntelTools\openvino\openvino_2021\deployment_tools\inference_engine\external\tbb\bin
C:\IntelTools\openvino\openvino_2021\deployment_tools\ngraph\lib
C:\Users\NewAdmin\Documents\Intel\OpenVINO\omz_demos_build\intel64\Debug
C:\Users\NewAdmin\Documents\Intel\OpenVINO\omz_demos_build\intel64\Release
```
![](Images/Path.png)
<center>图5 Path</center>

- 下载模型文件
这个路径下的downloader.py文件可以用于下载模型文件：
```
C:\IntelTools\openvino\openvino_2021\deployment_tools\tools\model_downloader
```
具体模型名称，可以在以下路径查看：
```
C:\IntelTools\openvino\openvino_2021\deployment_tools\intel_models
```
![](Images/model.png)
<center>图6 model</center>
由于我们要执行的 demo 是 human_pose_estimation_demo.exe ，所以我们需要下载 human-pose-estimation-0001 模型
```
命令：
python C:\IntelTools\openvino\openvino_2021\deployment_tools\tools\model_downloader\downloader.py --name human_pose_estimation_demo
```

![](Images/download_model.png)
<center>图7 download_model</center>

- 下载成功后，会在C盘的Intel文件夹下看到human-pose-estimation-0001的文件夹
![](Images/model_success.png)
<center>图8 model_success</center>

#### 4.执行程序
- 进入以下路径的文件夹
```
cd C:\Users\NewAdmin\Documents\Intel\OpenVINO\omz_demos_build\intel64\Debug
```
![](Images/run_exe.png)
<center>图9 run_exe</center>
- 输入以下命令
```
human_pose_estimation_demo.exe -i C:\Users\NewAdmin\Videos\6.mp4 -m C:\intel\human-pose-estimation-0001\FP32\human-pose-estimation-0001.xml -d CPU
```

![](Images/cmd_exe.png)
<center>图10 cmd_exe</center>

#### OpenVINO测试样例如下图所示
![](Images/7.png)
<center>图11 OpenVINO测试样例</center>


## 3 实验结果

##### 这是我于2020年12月25日在武汉天地东风雪铁龙举办的活动现场录制的雪铁龙Ami纯电动车行驶在模拟交通道路中的视频，我用该视频作为测试样例。

![](/images/test1.png)

![](/images/test3.png)

![](/images/test2.png)

![](/images/test4.png)
![](/images/test5.png)
![](/images/test6.png)

#### 通过以上测试样例截图可以看出，使用OpenVINO成功实现了行人动作感知，画面中明确标注出了行人的姿态与动作，颜色区分鲜明。

## 4 实验代码

```C++
main.cpp
// Copyright (C) 2018-2019 Intel Corporation
// SPDX-License-Identifier: Apache-2.0
//

/**
* \brief The entry point for the Inference Engine Human Pose Estimation demo application
* \file human_pose_estimation_demo/main.cpp
* \example human_pose_estimation_demo/main.cpp
*/

#include <vector>
#include <chrono>

#include <inference_engine.hpp>

#include <monitors/presenter.h>
#include <samples/images_capture.h>
#include <samples/ocv_common.hpp>

#include "human_pose_estimation_demo.hpp"
#include "human_pose_estimator.hpp"
#include "render_human_pose.hpp"

using namespace InferenceEngine;
using namespace human_pose_estimation;

bool ParseAndCheckCommandLine(int argc, char* argv[]) {
    // ---------------------------Parsing and validation of input args--------------------------------------

    gflags::ParseCommandLineNonHelpFlags(&argc, &argv, true);
    if (FLAGS_h) {
        showUsage();
        showAvailableDevices();
        return false;
    }

    std::cout << "Parsing input parameters" << std::endl;

    if (FLAGS_i.empty()) {
        throw std::logic_error("Parameter -i is not set");
    }

    if (FLAGS_m.empty()) {
        throw std::logic_error("Parameter -m is not set");
    }

    return true;
}

int main(int argc, char* argv[]) {
    try {
        std::cout << "InferenceEngine: " << printable(*GetInferenceEngineVersion()) << std::endl;

        // ------------------------------ Parsing and validation of input args ---------------------------------
        if (!ParseAndCheckCommandLine(argc, argv)) {
            return EXIT_SUCCESS;
        }

        HumanPoseEstimator estimator(FLAGS_m, FLAGS_d, FLAGS_pc);

        std::unique_ptr<ImagesCapture> cap = openImagesCapture(FLAGS_i, FLAGS_loop);
        cv::Mat curr_frame = cap->read();
        if (!curr_frame.data) {
            throw std::runtime_error("Can't read an image from the input");
        }

        cv::Size graphSize{curr_frame.cols / 4, 60};
        Presenter presenter(FLAGS_u, curr_frame.rows - graphSize.height - 10, graphSize);

        estimator.reshape(curr_frame);  // Do not measure network reshape, if it happened

        std::cout << "To close the application, press 'CTRL+C' here";
        if (!FLAGS_no_show) {
            std::cout << " or switch to the output window and press ESC key" << std::endl;
            std::cout << "To pause execution, switch to the output window and press 'p' key" << std::endl;
        }
        std::cout << std::endl;

        int delay = 1;
        bool isAsyncMode = false; // execution is always started in SYNC mode
        bool isModeChanged = false; // set to true when execution mode is changed (SYNC<->ASYNC)
        bool blackBackground = FLAGS_black;

        typedef std::chrono::duration<double, std::ratio<1, 1000>> ms;
        auto total_t0 = std::chrono::high_resolution_clock::now();
        auto wallclock = std::chrono::high_resolution_clock::now();
        double render_time = 0;
        do {
            auto t0 = std::chrono::high_resolution_clock::now();
            //here is the first asynchronus point:
            //in the async mode we capture frame to populate the NEXT infer request
            //in the regular mode we capture frame to the current infer request

            cv::Mat next_frame = cap->read();
            if (isAsyncMode) {
                if (isModeChanged) {
                    estimator.frameToBlobCurr(curr_frame);
                }
                if (next_frame.data) {
                    estimator.frameToBlobNext(next_frame);
                }
            } else if (!isModeChanged) {
                estimator.frameToBlobCurr(curr_frame);
            }
            auto t1 = std::chrono::high_resolution_clock::now();
            double decode_time = std::chrono::duration_cast<ms>(t1 - t0).count();

            t0 = std::chrono::high_resolution_clock::now();
            // Main sync point:
            // in the truly Async mode we start the NEXT infer request, while waiting for the CURRENT to complete
            // in the regular mode we start the CURRENT request and immediately wait for it's completion
            if (isAsyncMode) {
                if (isModeChanged) {
                    estimator.startCurr();
                }
                if (next_frame.data) {
                    estimator.startNext();
                }
            } else if (!isModeChanged) {
                estimator.startCurr();
            }

            estimator.waitCurr();
            t1 = std::chrono::high_resolution_clock::now();
            ms detection = std::chrono::duration_cast<ms>(t1 - t0);
            t0 = std::chrono::high_resolution_clock::now();
            ms wall = std::chrono::duration_cast<ms>(t0 - wallclock);
            wallclock = t0;

            t0 = std::chrono::high_resolution_clock::now();

            if (blackBackground) {
                curr_frame = cv::Mat::zeros(curr_frame.size(), curr_frame.type());
            }
            std::ostringstream out;
            out << "OpenCV cap/render time: " << std::fixed << std::setprecision(1)
                << (decode_time + render_time) << " ms";
            cv::putText(curr_frame, out.str(), cv::Point2f(0, 25),
                        cv::FONT_HERSHEY_TRIPLEX, 0.6, cv::Scalar(0, 255, 0));
            out.str("");
            out << "Wallclock time " << (isAsyncMode ? "(TRUE ASYNC):      " : "(SYNC, press Tab): ");
            out << std::fixed << std::setprecision(2) << wall.count()
                << " ms (" << 1000.0 / wall.count() << " fps)";
            cv::putText(curr_frame, out.str(), cv::Point2f(0, 50),
                        cv::FONT_HERSHEY_TRIPLEX, 0.6, cv::Scalar(0, 0, 255));
            if (!isAsyncMode) {  // In the true async mode, there is no way to measure detection time directly
                out.str("");
                out << "Detection time  : " << std::fixed << std::setprecision(1) << detection.count()
                << " ms ("
                << 1000.0 / detection.count() << " fps)";
                cv::putText(curr_frame, out.str(), cv::Point2f(0, 75), cv::FONT_HERSHEY_TRIPLEX, 0.6,
                    cv::Scalar(255, 0, 0));
            }

            std::vector<HumanPose> poses = estimator.postprocessCurr();

            if (FLAGS_r) {
                if (!poses.empty()) {
                    std::time_t result = std::time(nullptr);
                    char timeString[sizeof("2020-01-01 00:00:00: ")];
                    std::strftime(timeString, sizeof(timeString), "%Y-%m-%d %H:%M:%S: ", std::localtime(&result));
                    std::cout << timeString;
                    }

                for (HumanPose const& pose : poses) {
                    std::stringstream rawPose;
                    rawPose << std::fixed << std::setprecision(0);
                    for (auto const& keypoint : pose.keypoints) {
                        rawPose << keypoint.x << "," << keypoint.y << " ";
                    }
                    rawPose << pose.score;
                    std::cout << rawPose.str() << std::endl;
                }
            }

            presenter.drawGraphs(curr_frame);
            renderHumanPose(poses, curr_frame);

            if (!FLAGS_no_show) {
                cv::imshow("Human Pose Estimation on " + FLAGS_d, curr_frame);
            }

            t1 = std::chrono::high_resolution_clock::now();
            render_time = std::chrono::duration_cast<ms>(t1 - t0).count();

            if (isModeChanged) {
                isModeChanged = false;
            }

            // Final point:
            // in the truly Async mode we swap the NEXT and CURRENT requests for the next iteration
            curr_frame = std::move(next_frame);
            if (isAsyncMode) {
                estimator.swapRequest();
            }

            if (!FLAGS_no_show) {
                const int key = cv::waitKey(delay) & 255;
                if (key == 'p') {
                    delay = (delay == 0) ? 1 : 0;
                } else if (27 == key) { // Esc
                    break;
                } else if (9 == key) { // Tab
                    isAsyncMode = !isAsyncMode;
                    isModeChanged = true;
                } else if (32 == key) { // Space
                    blackBackground = !blackBackground;
                }
                presenter.handleKey(key);
            }
        } while (curr_frame.data);

        auto total_t1 = std::chrono::high_resolution_clock::now();
        ms total = std::chrono::duration_cast<ms>(total_t1 - total_t0);
        std::cout << "Total Inference time: " << total.count() << std::endl;
        std::cout << presenter.reportMeans() << '\n';
    }
    catch (const std::exception& error) {
        std::cerr << "[ ERROR ] " << error.what() << std::endl;
        return EXIT_FAILURE;
    }
    catch (...) {
        std::cerr << "[ ERROR ] Unknown/internal exception happened." << std::endl;
        return EXIT_FAILURE;
    }

    std::cout << "Execution successful" << std::endl;
    return EXIT_SUCCESS;
}
```

## 5 实验总结
&nbsp; 当前，智能网联汽车的定义是将人工智能技术、互联网技术与汽车相结合。智能汽车研发的基础，也就是自动驾驶汽车，首先要做到汽车自身的“主动安全”，拥有强有力的主动判决机制与主动制动技术将会为自动驾驶研发减少困难、降低研发成本。本实验通过OpenVINO与车载摄像头简单实现行人动作感知预测，有效地帮助自动驾驶汽车提高其主动安全性能。

![](/images/9.jpg)
<center>图12 基于视觉对汽车、行人进行感知的自动驾驶汽车</center>

&nbsp; 



&nbsp; 通过本次实验，我学习了通过OpenVINO实现行人动作感知的相关知识。通过分析源代码可知OpenVINO开发人员实现该技术的相关方式与开放算法流程。通过对行人的面部特征与肢体等进行行人识别及动作预测，能够较为准确的在光线充足且要素充足的条件下进行识别，能够通过相应预测行人即将发生的肢体动作进行预判。

#### <center>实验分析与心得体会</center>
&nbsp; 通过本次“使用OpenVINO对行人进行感知预测实验”我初步实现对行人动作进行感知的流程，系统能够监测到视频中实时出现的行人并标明行人的肢体轮廓与动作，从而做出行人动作预测（human pose estimation）。在基本实现行人动作预测的基础上：通过完成本次的实验步骤，我熟练掌握了Visual Studio Community软件的使用方式与程序配置，当实验遇到困难和问题时，我学会自主发现问题并解决问题的方法（例如将错误代码复制至网页中寻找解决方法）。这次课程实验，令我受益匪浅，也为我未来的学习与工作，提供了相应的思路。

## 参考文献
1.https://www.autohome.com.cn/use/201809/922574.html， 能防“鬼探头” C-V2X技术这么厉害？ ，汽车之家
2.https://blog.csdn.net/u011385476/article/details/104769250， OpenVINO 学习笔记：执行 OpenVINO Demos，CSDN
3.https://blog.csdn.net/u011385476/article/details/104789995， 配置 human_pose_estimation_demo 的开发环境，CSDN
4.https://www.mynewsdesk.com/uk/thatcham-research/videos/euro-ncap-demonstration-pedestrian-autonomous-emergency-braking-26955， 欧洲NCAP新举措，Euro NCAP
